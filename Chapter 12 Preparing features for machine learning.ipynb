{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71349ff1-3dd6-426e-bdca-3ded736ebb35",
   "metadata": {},
   "source": [
    "# Chapter 12 Setting the stage: Preparing features for machine learning\n",
    "we will disucss\n",
    "- How investing in a solid data manipulation foundation makes data preparation a breeze\n",
    "- Addressing big data quality problems with PySpark\n",
    "- Creating custom features for your ML model\n",
    "- Selecting compelling features for your model\n",
    "- Using transformers and estimators as part of the feature engineering process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3dd15c-7a42-453c-b155-3aadec41fd2c",
   "metadata": {},
   "source": [
    "## Main Step for Data Cleaning and Pro-processing\n",
    "\n",
    "- standardizing column names\n",
    "- check data types and correct mismatching data type\n",
    "- deal with missing values\n",
    "- deal with outliers\n",
    "- deal with correlations among features\n",
    "- deal with the feaures with raw occurance\n",
    "- standarize/normalize features\n",
    "- feature creation/engineering\n",
    "- fearure refinement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98dd82f1-09e6-4cc7-8654-463f9f295a54",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# change the account name to your email account\n",
    "account='sli'\n",
    "\n",
    "# define a root path to access the data in the DataAnalysisWithPythonAndPySpark\n",
    "data_path='/net/clusterhn/home/'+account+'/isa460/data/'\n",
    "\n",
    "# append path to helper_functions to system path\n",
    "sys.path.append('/net/clusterhn/home/sli/isa460_sli')\n",
    "\n",
    "import helper_functions as H\n",
    "\n",
    "# check if the Spark session is active. If it is activate, close it\n",
    "\n",
    "try:\n",
    "    if spark:\n",
    "        spark.stop()\n",
    "except:\n",
    "    pass    \n",
    "\n",
    "spark = (SparkSession.builder.appName(\"Preparing Featurers for Machine Learning\")\n",
    "        .config(\"spark.port.maxRetries\", \"100\")\n",
    "        .config(\"spark.sql.mapKeyDedupPolicy\", \"LAST_WIN\")  # This configuration allow the duplicate keys in the map data type.\n",
    "        .config(\"spark.driver.memory\", \"8g\")\n",
    "        .getOrCreate())\n",
    "\n",
    "# confiture the log level (defaulty is WARN)\n",
    "spark.sparkContext.setLogLevel('ERROR')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7568293f-70a2-4b8c-845c-ca9c3f3c10e1",
   "metadata": {},
   "source": [
    "# Reading, exploring, and preparing our machine learning data set\n",
    "For our ML model, we chose a data set of 20,057 dish names that contain 680 columns characterizing the ingredient list, the nutritional content, and the category of the dish. Our goal here is to predict if this dish is a dessert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11fce21-2dbc-4d16-b403-2514ffe4ad4a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "food=spark.read.csv(data_path+'recipes/epi_r.csv', inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3912e9d6-30ad-48a5-b231-2148b708616b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(food.count(), len(food.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6f3304-bfda-4dd5-9896-a70d7867cb60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "food.select('title',\n",
    " 'rating',\n",
    " 'calories',\n",
    " 'protein',\n",
    " 'fat',\n",
    " 'sodium',\n",
    " '#cakeweek',\n",
    " '#wasteless',\n",
    "'dessert').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a378e73-53b3-4c47-93fb-749f8badf907",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "food.groupBy('dessert').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25dd5eb8-0782-45a4-b215-1d3167eebbd2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#food.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5459043-04d8-49af-93be-1c90436605d9",
   "metadata": {},
   "source": [
    "## Standardizing column names using toDF()\n",
    "We will remove anything that isnâ€™t a letter or a number, standardize the spaces and other separators to use the underscore (_) character, and replace the ampersand (&) with its English equivalent and."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612cd124-5838-4191-b520-f6511f35d164",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sanitize_column_name(name):\n",
    "    \"\"\"Drops unwanted characters from the column name.\n",
    " \n",
    "    We replace spaces, dashes and slashes with underscore,\n",
    "    and only keep alphanumeric characters.\n",
    "    \"\"\"\n",
    "    answer = name\n",
    "    for i, j in ((\" \", \"_\"), (\"-\", \"_\"), (\"/\", \"_\"), (\"&\", \"and\")):   \n",
    "        answer = answer.replace(i, j)\n",
    "    return \"\".join(\n",
    "        [\n",
    "            char\n",
    "            for char in answer\n",
    "            if char.isalpha() or char.isdigit() or char == \"_\"       \n",
    "        ]\n",
    "    )\n",
    "# use toDF() to apply functions to all columns in the data frame\n",
    "food1 = food.toDF(*[sanitize_column_name(name) for name in food.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f523109c-a0b1-48a9-995b-276ed73c0080",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#food1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aafc292-9ce7-4ddc-aa4e-4945109f8c0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Exercise: Modify the above functions to change all characters to lower case\n",
    "# and apply to the food data frame\n",
    "\n",
    "def sanitize_column_name(name):\n",
    "    \"\"\"Drops unwanted characters from the column name.\n",
    " \n",
    "    We replace spaces, dashes and slashes with underscore,\n",
    "    and only keep alphanumeric characters.\n",
    "    \"\"\"\n",
    "    answer = name.lower()\n",
    "    for i, j in ((\" \", \"_\"), (\"-\", \"_\"), (\"/\", \"_\"), (\"&\", \"and\")):   \n",
    "        answer = answer.replace(i, j)\n",
    "    return \"\".join(\n",
    "        [\n",
    "            char\n",
    "            for char in answer\n",
    "            if char.isalpha() or char.isdigit() or char == \"_\"       \n",
    "        ]\n",
    "    )\n",
    "# use toDF() to apply functions to all columns in the data frame\n",
    "food1 = food.toDF(*[sanitize_column_name(name) for name in food.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a215a9ac-9c12-4a35-8647-d4ce36694ce7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#food1.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147aa407-c524-4d8f-8a1b-e907e014c525",
   "metadata": {},
   "source": [
    "## Exploring our data and getting our first feature columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ac6042-6ba9-4dac-baca-d049646129ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# check the statistics of numerical columns. As an example, we only focuse on the first 20 columns\n",
    "for x in food.columns[0:20]:\n",
    "    if food.schema[x].dataType!=T.StringType():\n",
    "        food.select(x).summary().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa904ad9-c0fd-4659-b2fc-574cff3d5407",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "food.groupBy('anise').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c021e1b3-8dc1-4ee6-aea4-1825de776cc8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# check specific column\n",
    "\n",
    "food.select('clove').summary().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86603fe-fda6-43ed-b49c-6759e0cecb4f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# check all columns with binary value. As an example, we will only focus on the first 20 columns.\n",
    "import pandas as pd\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "is_binary = food1.agg(\n",
    "    *[(F.countDistinct(col) == 2).alias(col) for col in food1.columns[0:20]]\n",
    ").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808da046-0eb2-4638-a6eb-0ee711e088ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#is_binary\n",
    "is_binary.unstack()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1939f2-6267-4518-a399-44e55fa46082",
   "metadata": {},
   "source": [
    "## Addressing data misshaps, using cakeweek and wasterless as example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f036ff1-52a4-411f-9f31-ce25db0b185d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "food1.groupBy('cakeweek').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc218864-d5a9-4d7d-8408-c5b2387bf39b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "food1.groupBy('wasteless').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59126cf2-7cd8-43a4-ae20-2aa33161e076",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for cakeweek and wasteless, only keep 0, 1 and null. Drop the rest\n",
    "\n",
    "food2 = food1.where(\n",
    "    (\n",
    "        F.col(\"cakeweek\").isin([0.0, 1.0])    \n",
    "        | F.col(\"cakeweek\").isNull()         \n",
    "    )\n",
    "    & (\n",
    "        F.col(\"wasteless\").isin([0.0, 1.0])    \n",
    "        | F.col(\"wasteless\").isNull()         \n",
    "    )\n",
    ")\n",
    " \n",
    "print(food1.count(), food2.count())\n",
    " \n",
    "# result 20057 20054. Three records were removed.                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9987a47f-d1b8-44b1-8891-1546f9f6b551",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check category columns\n",
    "catColumns=[x for x in food2.columns if food2.schema[x].dataType==F.StringType()]\n",
    "\n",
    "catColumns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7744f72-d8ba-4208-9be6-6944ba3de663",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# rating and calories should be double. Covert the type from string to double\n",
    "\n",
    "food2=food2.withColumn('rating', F.col('rating').cast('Double')) \\\n",
    "     .withColumn('calories', F.col('calories').cast('Double'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6226f21-0806-4c22-b6e9-80d79fd2dc56",
   "metadata": {},
   "source": [
    "### Create four top level variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d4a193-cf44-4885-afd7-88eb6b848205",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "IDENTIFIERS = [\"title\"]\n",
    " \n",
    "CONTINUOUS_COLUMNS = [\n",
    "    \"rating\",\n",
    "    \"calories\",\n",
    "    \"protein\",\n",
    "    \"fat\",\n",
    "    \"sodium\",\n",
    "]\n",
    " \n",
    "TARGET_COLUMN = [\"dessert\"]\n",
    " \n",
    "BINARY_COLUMNS = [\n",
    "    x\n",
    "    for x in food2.columns\n",
    "    if x not in CONTINUOUS_COLUMNS\n",
    "    and x not in TARGET_COLUMN\n",
    "    and x not in IDENTIFIERS\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6a891d-349b-49ef-bb1d-97604dda12c4",
   "metadata": {},
   "source": [
    "## Deal with missing values (Weeding our useless records and imputing binary features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5016a8a2-56a0-4651-8a1f-aadaa036d6c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# drop records where all features are null\n",
    "\n",
    "food3 = food2.dropna(\n",
    "    how=\"all\",\n",
    "    subset=[x for x in food2.columns if x not in IDENTIFIERS]\n",
    ")\n",
    "\n",
    "# drop records whereh target column is null\n",
    "\n",
    "food3 = food3.dropna(subset=TARGET_COLUMN)\n",
    "\n",
    "# impute binary columns. Fill null value with 0\n",
    "\n",
    "food3=food3.fillna(0.0, subset=BINARY_COLUMNS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3c4577-3647-4afe-8055-7da86b13f4ac",
   "metadata": {},
   "source": [
    "## Take care of extreme values/outliners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53eba8a4-bcbb-4440-9731-b0157f6b7f11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "food3.select(*CONTINUOUS_COLUMNS).summary().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344f2c4f-d491-4319-95a1-9e74f2b8968c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# return 99 percentile of each feature\n",
    "\n",
    "selected_columns=[ \"calories\", \"protein\", \"fat\",\"sodium\"]\n",
    "\n",
    "maximum={}\n",
    "\n",
    "for c in selected_columns:\n",
    "    maximum[c]=food3.select(F.percentile_approx(c, 0.99)).collect()[0][0]\n",
    "    \n",
    "maximum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b0f466-a420-472c-829a-712d77336d82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for the above feature, replace any value over 99 percentile to the value at 99 percentile\n",
    "\n",
    "for k, v in maximum.items():\n",
    "    food3=food3.withColumn(k, F.when(F.isnull(F.col(k)), F.col(k)).otherwise(\n",
    "        F.least(F.col(k), F.lit(v))\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9818c9b3-a6c0-4c4a-85d4-4c61639c3262",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Check the boxplot of each feature\n",
    "\n",
    "selected_columns=[ \"calories\", \"protein\", \"fat\",\"sodium\"]\n",
    "\n",
    "# without removing outliners\n",
    "#df=food2.select(selected_columns).toPandas()\n",
    "\n",
    "# with outliners removed\n",
    "df=food3.select(selected_columns).toPandas()\n",
    "\n",
    "# Set up a 4X1 grid of subplots\n",
    "fig, axes = plt.subplots(4, 1, figsize=(8, 14))\n",
    "\n",
    "i=0\n",
    "for c in selected_columns:\n",
    "    sns.boxplot(x=df[c], ax=axes[i])\n",
    "    i=i+1\n",
    "    \n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6568c129-330b-4380-8907-47c1ace8ef85",
   "metadata": {},
   "source": [
    "## Weeding out the rare binary occurrence columns\n",
    "\n",
    "Binary features with only a few zeroes or ones are not helpful in classifying a recipe as a dessert: if every recipe (or no recipe) has a certain feature as true, then that feature does not discriminate properly, meaning that our model has no use for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac9ef35-5af3-43c1-a1f0-ff418434e98e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for binary variables, remove features with less then 10 of 0 or 1.\n",
    "\n",
    "inst_sum_of_binary_columns = [\n",
    "    F.sum(F.col(x)).alias(x) for x in BINARY_COLUMNS\n",
    "]\n",
    " \n",
    "sum_of_binary_columns = (\n",
    "    food3.select(*inst_sum_of_binary_columns).head().asDict()         \n",
    ")\n",
    " \n",
    "num_rows = food3.count()\n",
    "too_rare_features = [\n",
    "    k\n",
    "    for k, v in sum_of_binary_columns.items()\n",
    "    if v < 10 or v > (num_rows - 10)\n",
    "]\n",
    "\n",
    "BINARY_COLUMNS = list(set(BINARY_COLUMNS) - set(too_rare_features)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5641fdc-6c1a-4757-a9c5-21eeae131294",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#too_rare_features\n",
    "\n",
    "len(too_rare_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35bd40fb-f4c4-4cd6-9483-d0ebef7640fb",
   "metadata": {},
   "source": [
    "# Feature Creation/Engineering and Refinement\n",
    "\n",
    "- Creating a few custom features using our continuous feature columns\n",
    "\n",
    "- Measuring correlation over original and generated continuous features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb70ad1-1f55-40f4-968c-d3b88277ce80",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Feaure Creation\n",
    "As an example, weâ€™ll take the protein and fat columns representing the quantity (in grams) of protein and fat in the recipe, respectively. With the information in those two columns, we create two features representing the percentage of calories attributed to each macro nutriment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3886fdc-59be-46d0-9a26-241b3205f6d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "food3 = food3.withColumn(\n",
    "    \"protein_ratio\", F.col(\"protein\") * 4 / F.col(\"calories\")\n",
    ").withColumn(\n",
    "    \"fat_ratio\", F.col(\"fat\") * 9 / F.col(\"calories\")\n",
    ")                                                           \n",
    " \n",
    "food3 = food3.fillna(0.0, subset=[\"protein_ratio\", \"fat_ratio\"])\n",
    " \n",
    "CONTINUOUS_COLUMNS += [\"protein_ratio\", \"fat_ratio\"]    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0df71fb-e091-41b7-a766-fa326ead91aa",
   "metadata": {},
   "source": [
    "## Removing highly correlated features\n",
    "\n",
    "We want our features to be correlated with our target (this provides predictive power). On the other hand, we want to avoid correlation between features for two main reasons:\n",
    "- If two features are highly correlated, it means that they provide almost the same information. In the context of machine learning, this can confuse the fitting algorithm and create model or numerical instability.\n",
    "- The more complex your model, the more complex the maintenance. Highly correlated features rarely provide improved accuracy, yet complicate the model. Simple is better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c622f4-af6c-4e62-a428-84b262fce170",
   "metadata": {},
   "source": [
    "### convert to pandas to calculate correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16acaf5d-3cf3-4767-9dce-40e8366b837d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "corr=food3.select(CONTINUOUS_COLUMNS).toPandas().corr()\n",
    "corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01299856-c435-47c6-b814-6617a66d1778",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(corr, annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "plt.title('Correlation Heatmap')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67b4205-8dd2-4cbf-b3cc-ff4e9e0b49a4",
   "metadata": {},
   "source": [
    "### Use pyspark way to calcualte correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f53bda-a708-4744-b0df-d920ac065529",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    " \n",
    "continuous_features = VectorAssembler(\n",
    "    inputCols=CONTINUOUS_COLUMNS, outputCol=\"continuous_features\"\n",
    ")\n",
    " \n",
    "vector_food = food3.select(CONTINUOUS_COLUMNS)\n",
    "for x in CONTINUOUS_COLUMNS:\n",
    "    vector_food = vector_food.where(~F.isnull(F.col(x))) \n",
    " \n",
    "vector_variable = continuous_features.transform(vector_food)\n",
    " \n",
    "vector_variable.select(\"continuous_features\").show(3, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3145cd6b-cb4d-4212-80e9-5a1583eb3169",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.stat import Correlation\n",
    " \n",
    "correlation = Correlation.corr(\n",
    "    vector_variable, \"continuous_features\"                \n",
    ")\n",
    " \n",
    "correlation_array = correlation.head()[0].toArray()       \n",
    " \n",
    "correlation_pd = pd.DataFrame(\n",
    "    correlation_array,                                      \n",
    "    index=CONTINUOUS_COLUMNS,                               \n",
    "    columns=CONTINUOUS_COLUMNS,                             \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea319df9-11ea-4b28-95e5-14a9e7be5fe6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(correlation_pd.iloc[:, :4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39fed27d-51be-4cf3-b8b1-12bfe06d62e9",
   "metadata": {},
   "source": [
    "## Feature preparation with transformers and estimators\n",
    "We explore two relevant examples of transformers and estimators:\n",
    "- Null imputation, where we provide a value to replace null occurrences in a column (e.g., the mean)\n",
    "- Scaling features, where we normalize the values of a column, so they are on a more logical scale (e.g., between zero and one)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7601fe48-15d2-4bb9-ba54-f8a6d0095974",
   "metadata": {},
   "source": [
    "[Function Approach and Transformer Approach](https://raw.githubusercontent.com/Suhong88/ISA460_Fall2023/main/images/Figure%2012.2.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73b1b5d-0fee-432d-9006-38fb3342cdee",
   "metadata": {},
   "source": [
    "### Imputing continuous features using the Imputer estimator\n",
    "\n",
    "At the core, an estimator is a transformer-generating object. We instantiate an estimator object just like a transformer by providing the relevant parameters to its constructor. To apply an estimator, we call the fit() method, which returns a Model object, which is, for all purposes, the same as a transformer. Estimators allow for automatically creating transformers that depend on the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f4da93-47f8-437e-8940-4392b3d970ce",
   "metadata": {},
   "source": [
    "[Imputer and its companion transformer/model](https://raw.githubusercontent.com/Suhong88/ISA460_Fall2023/main/images/Figure%2012.3.JPG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950df473-d404-44d5-8be5-a51fdde4ec98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# impute the following 5 featurs with mean\n",
    "\n",
    "from pyspark.ml.feature import Imputer\n",
    " \n",
    "OLD_COLS = [\"rating\", \"calories\", \"protein\", \"fat\", \"sodium\"]\n",
    "NEW_COLS = [\"rating_i\", \"calories_i\", \"protein_i\", \"fat_i\", \"sodium_i\"]\n",
    " \n",
    "imputer = Imputer(\n",
    "    strategy=\"mean\",                                            \n",
    "    inputCols=OLD_COLS,                                         \n",
    "    outputCols=NEW_COLS,                                        \n",
    ")\n",
    " \n",
    "imputer_model = imputer.fit(food3)  \n",
    "\n",
    "food_imputed = imputer_model.transform(food3)\n",
    " \n",
    "CONTINUOUS_COLUMNS = (\n",
    "    list(set(CONTINUOUS_COLUMNS) - set(OLD_COLS)) + NEW_COLS    \n",
    ")\n",
    "\n",
    "CONTINUOUS_COLUMNS=set(CONTINUOUS_COLUMNS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb100236-0da0-495b-aa32-0317ac2c1e2c",
   "metadata": {},
   "source": [
    "### Scaling our features using the MinMaxScaler estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc918d7-8540-45fd-8d93-aa5f4c72dc29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import MinMaxScaler\n",
    " \n",
    "CONTINUOUS_NB = [x for x in CONTINUOUS_COLUMNS if \"ratio\" not in x]\n",
    " \n",
    "continuous_assembler = VectorAssembler(\n",
    "    inputCols=CONTINUOUS_NB, outputCol=\"continuous\"\n",
    ")\n",
    " \n",
    "food_features = continuous_assembler.transform(food_imputed)\n",
    " \n",
    "continuous_scaler = MinMaxScaler(\n",
    "    inputCol=\"continuous\",\n",
    "    outputCol=\"continuous_scaled\",\n",
    ")\n",
    " \n",
    "food_features = continuous_scaler.fit(food_features).transform(food_features)\n",
    "\n",
    "food_features.select(\"continuous_scaled\").show(3, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954ec36c-18b0-4180-b8a8-38624e6e1ade",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b989af6-74f7-4908-9b19-606f2d53539d",
   "metadata": {},
   "source": [
    "- big part of creating a machine learning model is data manipulation. For this, everything weâ€™ve learned within pyspark.sql can be leveraged.\n",
    "- The first step in creating an ML model is assessing data quality and addressing potential data problems. Moving from large data sets in PySpark to small summaries in pandas or plain Python speeds up data discovery and assessment.\n",
    "- Feature creation and selection can either be done manually using the PySpark data manipulation API or by leveraging some pyspark.ml-specific constructors, such as the correlation matrix.\n",
    "- Multiple common feature engineering patterns, like imputing and scaling features, are provided through PySpark transformers and estimators."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Big Data Analytics",
   "language": "python",
   "name": "bigdataanalytics"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "toc-autonumbering": true,
  "toc-showcode": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
