{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0e9370c-6575-4883-832e-0367085046d6",
   "metadata": {},
   "source": [
    "# Note:\n",
    "1. Create a directory isa460 under your home directory: mkdikr isa460\n",
    "2. change to isa460: cd isa460\n",
    "3. clone the [code](https://github.com/jonesberg/DataAnalysisWithPythonAndPySpark) and [data](https://github.com/jonesberg/DataAnalysisWithPythonAndPySpark-Data) for your textbook\n",
    "4. rename the directory DataAnalysisWithPythonAndPySpark-Data to data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d73e0a-1ca7-4b44-a453-7f06bc611d0c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# My First Pyspark Program: What are the most popular words used in the English language? (based on Jane Austen's Pride and Prejudice)\n",
    "\n",
    "1. Read—Read the input data (we’re assuming a plain text file).\n",
    "\n",
    "2. Token—Tokenize each word.\n",
    "\n",
    "3. Clean—Remove any punctuation and/or tokens that aren’t words. Lowercase each word.\n",
    "\n",
    "4. Count—Count the frequency of each word present in the text.\n",
    "\n",
    "5. Answer—Return the top 10 (or 20, 50, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c67a97b-8b34-4709-9c05-0efdd54a864b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/09/08 17:09:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# change the account name to your email account\n",
    "account='sli'\n",
    "\n",
    "# define a root path to access the data in the DataAnalysisWithPythonAndPySpark\n",
    "root_path='/net/clusterhn/home/'+account+'/isa460/Data/'\n",
    "\n",
    "# create a spark session\n",
    "spark = SparkSession.builder.appName(\"My First Spark Program\")\\\n",
    "        .config(\"spark.port.maxRetries\", \"100\")\\\n",
    "        .getOrCreate()\n",
    "\n",
    "# confiture the log level (defaulty is WWARN)\n",
    "spark.sparkContext.setLogLevel('ERROR')\n",
    "\n",
    "# read the csv file\n",
    "book = spark.read.text(root_path+\"gutenberg_books/1342-0.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b4c7f9-bdb8-49a5-b809-074b79e2848f",
   "metadata": {},
   "source": [
    "# Basic Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81251835-09f0-48ba-ae8f-69f36df1e526",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print the structure (or schema) of the data\n",
    "\n",
    "book.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ca093cd-4a25-4c34-a3d3-5cbdb86e625a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------+\n",
      "|                                             value|\n",
      "+--------------------------------------------------+\n",
      "|The Project Gutenberg EBook of Pride and Prejud...|\n",
      "|                                                  |\n",
      "|This eBook is for the use of anyone anywhere at...|\n",
      "|almost no restrictions whatsoever.  You may cop...|\n",
      "|re-use it under the terms of the Project Gutenb...|\n",
      "+--------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show a sample of data\n",
    "\n",
    "book.show(5, truncate=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92588492-2311-4a78-9cd2-aee2d9ad1fbd",
   "metadata": {},
   "source": [
    "## Simple column transformations: Moving from a sentence to a list of words\n",
    "\n",
    "### four ways to select a column\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    " \n",
    "- book.select(book.value)\n",
    "- book.select(book[\"value\"])\n",
    "- book.select(col(\"value\"))\n",
    "- book.select(\"value\")\n",
    "\n",
    "### Rename a column: alias() or withColumnRenamed()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21728b96-ef71-4573-9131-6f2225706a40",
   "metadata": {
    "tags": []
   },
   "source": [
    "## split the text into a list of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8e4c89-451f-464a-a751-f557cf258340",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1beaa5ac-1ed2-4c5f-87bb-9929f8fe3fc7",
   "metadata": {},
   "source": [
    "## exploding a list into rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60eb8d68-9d17-4821-a16d-818aaff5dbc3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f446b9a0-7092-45aa-8992-fdd8aefc8c59",
   "metadata": {},
   "source": [
    "## Working with words: changing case and removing puncutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00593c75-27a4-4dda-8d92-4d6d6e1cdf0b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7f9d0fc6-4f6e-4eb8-9890-d4bcd9af0d62",
   "metadata": {
    "tags": []
   },
   "source": [
    "## use regexp_extract to remove special characters\n",
    "\n",
    "### [Regular Expression Reference](https://docs.python.org/3/howto/regex.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35235602-86a5-47e4-88c1-297c3d4ce0a5",
   "metadata": {},
   "source": [
    "## Filtering Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff2bef5-e51d-4b40-a89c-05ed28d3c25d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2e66d4ac-e9be-48c4-b1c0-cd84d1606d57",
   "metadata": {},
   "source": [
    "## Grouping records: Counting word frequencies\n",
    "\n",
    "![GroupedData](https://raw.githubusercontent.com/Suhong88/ISA460_Fall2023/main/images/3.1.A.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba10b213-52c2-4b33-a378-d7991d472cbb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f185f16e-6639-499d-9886-9de048a8e85d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Display number of words per letter count\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173398a4-7d18-4e63-a713-a4dc5e86e4ed",
   "metadata": {},
   "source": [
    "## Ordering the results using orderBy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a947682-3637-477d-b934-2c8daad748a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b774dad-93b8-4fdf-99d8-e5b1528242a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "132b21e8-50ed-4d6a-9dac-87d56b9d2f07",
   "metadata": {},
   "source": [
    "## Writing data from a data frame\n",
    "\n",
    "The results will be written in a directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "895c0ab4-13ae-4d88-8378-308cc17cd15b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e567ac86-c490-464e-8c80-3b44d40ce1ca",
   "metadata": {},
   "source": [
    "## Putting all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15d6e35b-4e17-4786-a367-836c67e0241f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|word|count|\n",
      "+----+-----+\n",
      "| the| 4480|\n",
      "|  to| 4218|\n",
      "|  of| 3711|\n",
      "| and| 3504|\n",
      "| her| 2199|\n",
      "|   a| 1982|\n",
      "|  in| 1909|\n",
      "| was| 1838|\n",
      "|   i| 1749|\n",
      "| she| 1668|\n",
      "+----+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col,\n",
    "    explode,\n",
    "    lower,\n",
    "    regexp_extract,\n",
    "    split,\n",
    ")\n",
    "\n",
    "# change the account name to your email account\n",
    "account='sli'\n",
    "\n",
    "# define a root path to access the data in the DataAnalysisWithPythonAndPySpark\n",
    "root_path='/net/clusterhn/home/'+account+'/isa460/Data/'\n",
    "\n",
    "spark = SparkSession.builder.appName(\n",
    "    \"Analyzing the vocabulary of Pride and Prejudice.\"\n",
    ").config(\"spark.port.maxRetries\", \"100\").getOrCreate()\n",
    " \n",
    "book = spark.read.text(root_path+\"gutenberg_books/1342-0.txt\")\n",
    " \n",
    "lines = book.select(split(book.value, \" \").alias(\"line\"))\n",
    " \n",
    "words = lines.select(explode(col(\"line\")).alias(\"word\"))\n",
    " \n",
    "words_lower = words.select(lower(col(\"word\")).alias(\"word\"))\n",
    " \n",
    "words_clean = words_lower.select(\n",
    "    regexp_extract(col(\"word\"), \"[a-z']*\", 0).alias(\"word\")\n",
    ")\n",
    " \n",
    "words_nonull = words_clean.where(col(\"word\") != \"\")\n",
    " \n",
    "results = words_nonull.groupby(col(\"word\")).count()\n",
    " \n",
    "results.orderBy(\"count\", ascending=False).show(10)\n",
    "\n",
    "results.coalesce(1).write.mode('overwrite').csv(\"simple_count_single_partition.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7496feda-5360-4f50-8378-fc7d50100483",
   "metadata": {},
   "source": [
    "## Simplying our program via method chaining\n",
    "![Method Chaining 3.3](https://raw.githubusercontent.com/Suhong88/ISA460_Fall2023/main/images/3.3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "971e5a92-14f5-43ef-932a-7fe228f956a7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n",
      "|         word|count|\n",
      "+-------------+-----+\n",
      "|       online|    4|\n",
      "|         some|  203|\n",
      "|        still|   72|\n",
      "|          few|   72|\n",
      "|         hope|  122|\n",
      "|        those|   60|\n",
      "|     cautious|    4|\n",
      "|       lady's|    8|\n",
      "|    imitation|    1|\n",
      "|          art|    3|\n",
      "|      solaced|    1|\n",
      "|       poetry|    2|\n",
      "|    arguments|    5|\n",
      "| premeditated|    1|\n",
      "|      elevate|    1|\n",
      "|       doubts|    2|\n",
      "|    destitute|    1|\n",
      "|    solemnity|    5|\n",
      "|gratification|    1|\n",
      "|    connected|   14|\n",
      "+-------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    " \n",
    "results = (\n",
    "    spark.read.text(root_path+\"gutenberg_books/1342-0.txt\")\n",
    "    .select(F.split(F.col(\"value\"), \" \").alias(\"line\"))\n",
    "    .select(F.explode(F.col(\"line\")).alias(\"word\"))\n",
    "    .select(F.lower(F.col(\"word\")).alias(\"word\"))\n",
    "    .select(F.regexp_extract(F.col(\"word\"), \"[a-z']*\", 0).alias(\"word\"))\n",
    "    .where(F.col(\"word\") != \"\")\n",
    "    .groupby(\"word\")\n",
    "    .count()\n",
    ")\n",
    "\n",
    "results.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe3edf6-6316-44e6-807b-c635b0aa70e0",
   "metadata": {},
   "source": [
    "## Using spark-submit to launc your program in batch mode\n",
    "\n",
    "- go to terminal\n",
    "- activate BigDataAnalytics environment: conda activate BigDataAnalysis\n",
    "- move to the directory where you have word_count_submit.py file\n",
    "- run the following code: spark-submit word_count_submit.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d38ecba-6ed5-48aa-ae29-e3101e4e0d99",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Big Data Analytics",
   "language": "python",
   "name": "bigdataanalytics"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
