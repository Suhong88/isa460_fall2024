{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27152ce5-9914-4d5f-a025-d15d58672bd2",
   "metadata": {},
   "source": [
    "# Chapter 5 Data Frame Gymnastics: Joining and Group\n",
    "This chapter covers\n",
    "- Joining two data frames together\n",
    "- Selecting the right type of join for your use case\n",
    "- Grouping data and understanding the GroupedData transitional object\n",
    "- Breaking the GroupedData with an aggregation method\n",
    "- Filling null values in your data frame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dff8f93-f3db-4609-a026-c086fc1e27c3",
   "metadata": {},
   "source": [
    "## Start a spark session and import logs and logs identifier table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c47edc-09e2-4290-8a85-b6b2adb83668",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "import os\n",
    "\n",
    "# change the account name to your email account\n",
    "account='sli'\n",
    "\n",
    "# define a root path to access the data in the DataAnalysisWithPythonAndPySpark\n",
    "root_path='/net/clusterhn/home/'+account+'/isa460/data/'\n",
    "\n",
    "spark = (SparkSession.builder.appName(\"Analyzing tabluar data\")\n",
    "        .config(\"spark.port.maxRetries\", \"100\")\n",
    "        .getOrCreate())\n",
    "\n",
    "# confiture the log level (defaulty is WWARN)\n",
    "spark.sparkContext.setLogLevel('ERROR')\n",
    "\n",
    "# import log file\n",
    "directory=root_path+'/broadcast_logs/'\n",
    "\n",
    "logs=spark.read.csv(os.path.join(directory, \"BroadcastLogs_2018_Q3_M8_sample.CSV\"),\n",
    "                                 sep=\"|\",\n",
    "                                 header=True,\n",
    "                                 inferSchema=True,\n",
    "                                 timestampFormat=\"yyyy-MM-dd\",)\n",
    "\n",
    "# add Duration seconds column\n",
    "logs=logs.withColumn(\"Duration_seconds\", F.col(\"Duration\").substr(1,2).cast(\"int\").alias(\"dur_hours\")*60*60+ \n",
    "            F.col(\"Duration\").substr(4,2).cast(\"int\").alias(\"dur_minutes\")*60+\n",
    "            F.col(\"Duration\").substr(7,2).cast(\"int\").alias(\"dur_seconds\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0544091-68ee-4bd4-859a-8f09e68494c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import log identifier table. We only want primary channel (PrimaryFG is 1)\n",
    "\n",
    "log_identifier=spark.read.csv(os.path.join(directory, \"ReferenceTables/LogIdentifier.csv\"),\n",
    "                                 sep=\"|\",\n",
    "                                 header=True,\n",
    "                                 inferSchema=True).filter(F.col('PrimaryFG')==1)\n",
    "log_identifier.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94975474-f602-4bc5-9069-1596779113b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "log_identifier.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3cd542-19c8-4471-8f36-b8795716c391",
   "metadata": {},
   "source": [
    "## Joining tables\n",
    "- Type of join: inner, left, right, full/outer, left_semi, left_anti, cross\n",
    "\n",
    "- A left semi-join (how=\"left_semi\") is the same as an inner join, but keeps the columns in the left table.\n",
    "- A left anti-join (how=\"left_anti\") is the opposite of an inner join. It will keep only the records from the left table that do not match the predicate with any record in the right table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc430995-1bba-46c3-b477-1dd40629006f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# join logs and channels tables. inner join.\n",
    "\n",
    "logs_and_channels=logs.join(log_identifier, on=\"LogServiceID\", how=\"inner\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314bdaea-33e6-4676-a4c6-c80306194fb5",
   "metadata": {},
   "source": [
    "## Naming conventions in the joning world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3dd42a1-83f2-409a-a097-e8a5851fe2cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# what happens if we join the tables with same column name\n",
    "\n",
    "logs_and_channels_verbose = logs.join(log_identifier, logs[\"LogServiceID\"] == log_identifier[\"LogServiceID\"])\n",
    "\n",
    "logs_and_channels_verbose.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f335e6c-2777-4b91-9b75-6047ff380f43",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.utils import AnalysisException\n",
    "\n",
    "try:\n",
    "    logs_and_channels_verbose.select(\"LogServiceID\")\n",
    "except AnalysisException as err:\n",
    "    print(err)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef208e9-e298-4a9b-b23a-f647bf53946d",
   "metadata": {},
   "source": [
    "### Note:\n",
    "PySpark happily joins the two data frames but fails when we try to work with the ambiguous column. This is a common situation when working with data that follows the same convention for column naming. To solve this problem, in this section I show three methods, from the easiest to the most general."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74f50a5-0446-4636-bc56-364b00207042",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# method 1. \n",
    "# use the following join. PySpark kept only the first referred column\n",
    "\n",
    "logs_and_channels = logs.join(log_identifier, \"LogServiceID\")\n",
    " \n",
    "logs_and_channels.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa15459e-7c83-4ad4-94dd-28749cd56155",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# method 2. Refer each column by adding table name. Drop one of the columns with the same name.\n",
    "\n",
    "logs_and_channels_verbose = logs.join(\n",
    "    log_identifier, logs[\"LogServiceID\"] == log_identifier[\"LogServiceID\"]\n",
    ")\n",
    " \n",
    "logs_and_channels.drop(log_identifier[\"LogServiceID\"]).select(\n",
    "    \"LogServiceID\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf273d0-8c21-4f45-ac52-13a80f69c2e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd096c85-8e2c-4505-bde6-a8361f74f20f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# method 3. alias() our tables when performing the join\n",
    "\n",
    "logs_and_channels_verbose = logs.alias(\"left\").join(\n",
    "    log_identifier.alias(\"right\"),                     \n",
    "    logs[\"LogServiceID\"] == log_identifier[\"LogServiceID\"],\n",
    ")\n",
    " \n",
    "logs_and_channels_verbose.drop(F.col(\"right.LogServiceID\")).select(\n",
    "    \"LogServiceID\"\n",
    ")             "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e7bfdd-7d91-480c-b8c8-7debcbc2fc0f",
   "metadata": {},
   "source": [
    "### join two more tables\n",
    "\n",
    "we will link two additional tables to continue our data discovery and processing. The CategoryID table contains information about the types of programs, and the ProgramClassID table contains the data that allows us to pinpoint the commercials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5513ba52-8578-451e-b203-10748fe78abc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import log file\n",
    "directory=root_path+'/broadcast_logs/'\n",
    "\n",
    "logs=spark.read.csv(os.path.join(directory, \"BroadcastLogs_2018_Q3_M8_sample.CSV\"),\n",
    "                                 sep=\"|\",\n",
    "                                 header=True,\n",
    "                                 inferSchema=True,\n",
    "                                 timestampFormat=\"yyyy-MM-dd\",)\n",
    "\n",
    "# add Duration second column\n",
    "logs=logs.withColumn(\"Duration_seconds\", F.col(\"Duration\").substr(1,2).cast(\"int\").alias(\"dur_hours\")*60*60+ \n",
    "            F.col(\"Duration\").substr(4,2).cast(\"int\").alias(\"dur_minutes\")*60+\n",
    "            F.col(\"Duration\").substr(7,2).cast(\"int\").alias(\"dur_seconds\"))\n",
    "\n",
    "# import log identifier\n",
    "log_identifier=spark.read.csv(os.path.join(directory, \"ReferenceTables/LogIdentifier.csv\"),\n",
    "                                 sep=\"|\",\n",
    "                                 header=True,\n",
    "                                 inferSchema=True).filter(F.col('PrimaryFG')==1)\n",
    "\n",
    "# join log and channel\n",
    "logs_and_channels = logs.join(log_identifier, \"LogServiceID\")\n",
    "\n",
    "# import category table and select needed columns\n",
    "cd_category = spark.read.csv(\n",
    "    os.path.join(directory, \"ReferenceTables/CD_Category.csv\"),\n",
    "    sep=\"|\",\n",
    "    header=True,\n",
    "    inferSchema=True,\n",
    ").select(\n",
    "    \"CategoryID\",\n",
    "    \"CategoryCD\",\n",
    "    F.col(\"EnglishDescription\").alias(\"Category_Description\"), \n",
    ")\n",
    "\n",
    "# import program class and select needed columns\n",
    "cd_program_class = spark.read.csv(\n",
    "    os.path.join(directory, \"ReferenceTables/CD_ProgramClass.csv\"),\n",
    "    sep=\"|\",\n",
    "    header=True,\n",
    "    inferSchema=True,\n",
    ").select(\n",
    "    \"ProgramClassID\",\n",
    "    \"ProgramClassCD\",\n",
    "    F.col(\"EnglishDescription\").alias(\"ProgramClass_Description\"),\n",
    ")\n",
    "\n",
    "# join log and channels tables with cd_category and cd_program_class tables\n",
    "\n",
    "full_log = logs_and_channels.join(cd_category, \"CategoryID\", how=\"left\").join(\n",
    "    cd_program_class, \"ProgramClassID\", how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9caa7c75-351b-4440-96cc-bafa68b43b26",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Summarizing the data via groupBy and GroupedData\n",
    "**what are the channels with the greatest and least proportion of commercials?**\n",
    "\n",
    "list of commercial codes: \"COM\", \"PRC\", \"PGI\", \"PRO\", \"PSA\", \"MAG\", \"LOC\", \"SPO\", \"MER\", \"SOL\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b48d622-5266-4ce5-858b-4c2c179bc0eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# display average program duration by program class\n",
    "\n",
    "(full_log.groupBy(\"ProgramClassCD\", \"ProgramClass_Description\")\n",
    " .agg(F.sum(\"Duration_seconds\").alias(\"duration_total\"))\n",
    " .orderBy(\"duration_total\",ascending=False)\n",
    ").show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7da5d2-907e-4129-a8f1-4f976cc627aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "log1=full_log.filter(F.col(\"ProgramClassCD\").isin([\"COM\", \"PRC\", \"PGI\", \"PRO\", \"PSA\", \"MAG\", \"LOC\", \"SPO\", \"MER\", \"SOL\"]))\n",
    "\n",
    "#log1=full_log.filter(F.trim(F.col(\"ProgramClassCD\")).isin([\"COM\", \"PRC\", \"PGI\", \"PRO\", \"PSA\", \"MAG\", \"LOC\", \"SPO\", \"MER\", \"SOL\"]))\n",
    "\n",
    "log1.select(\"ProgramClassCD\", \"ProgramClass_Description\").distinct().show(100,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cedfad6-710b-4b6c-9522-e1c9283a5281",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "answer = (\n",
    "    full_log.groupby(\"LogIdentifierID\")\n",
    "    .agg(\n",
    "        F.sum(                                                              \n",
    "            F.when(                                                        \n",
    "                F.trim(F.col(\"ProgramClassCD\")).isin(                       \n",
    "                    [\"COM\", \"PRC\", \"PGI\", \"PRO\", \"LOC\", \"SPO\", \"MER\", \"SOL\"]\n",
    "                ),                                                          \n",
    "                F.col(\"duration_seconds\"),                                  \n",
    "            ).otherwise(0)                                                  \n",
    "        ).alias(\"duration_commercial\"),                                     \n",
    "        F.sum(\"duration_seconds\").alias(\"duration_total\"),\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"commercial_ratio\", F.col(\n",
    "            \"duration_commercial\") / F.col(\"duration_total\")\n",
    "    )\n",
    ")\n",
    "\n",
    "answer.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5735e8-6c54-4302-88de-84cf806a1999",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# channels with the most commercial\n",
    "\n",
    "answer.orderBy(F.desc(\"commercial_ratio\")).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d075787-3bd3-4b5c-a164-9c17d4f6ce74",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# channel with the least commerical\n",
    "\n",
    "answer.orderBy(\"commercial_ratio\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77f18d2-a263-4120-85cf-2daca459d372",
   "metadata": {},
   "source": [
    "## Deal with null values\n",
    "dropna(), fillna()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde399ff-7c3e-4add-9c3f-8326e2958af3",
   "metadata": {},
   "source": [
    "### dropna()\n",
    "dropna() is pretty easy to use. This data frame method takes three parameters:\n",
    "\n",
    "- how, which can take the value any or all. If any is selected, PySpark will drop records where at least one of the fields is null. In the case of all, only the records where all fields are null will be removed. By default, PySpark will take the any mode.\n",
    "\n",
    "- thresh takes an integer value. If set (its default is None), PySpark will ignore the how parameter and only drop the records with less than thresh non-null values.\n",
    "\n",
    "- subset will take an optional list of columns that dropna() will use to make its decision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e743b89-759a-464a-b973-5965970fff69",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# drop the records that have commerical_ratio is null\n",
    "\n",
    "answer_no_null=answer.dropna(subset=[\"commercial_ratio\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c4ccc6-3527-49c5-b9a6-8eb43be6d750",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "answer_no_null.orderBy(\"commercial_ratio\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7206bc9-1163-442b-bfb4-0a063c46be21",
   "metadata": {},
   "source": [
    "### fillna()\n",
    "\n",
    "This data frame method takes two parameters:\n",
    "\n",
    "- The value, which is a Python int, float, string, or bool. PySpark will only fill the compatible columns; for instance, if we were to fillna(\"zero\"), our commercial_ratio, being a double, would not be filled.\n",
    "- The same subset parameter we encountered in dropna(). We can limit the scope of our filling to only the columns we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393d99e2-75a1-416a-973f-50c8ca3873ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nswer_no_null = answer.fillna(0)\n",
    " \n",
    "answer_no_null.orderBy(\n",
    "    \"commercial_ratio\", ascending=False).show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbab958-9032-4a5b-80e8-3f29efcf3e7c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Alternative method \n",
    "#Filling our numerical records with zero using the fillna() method and a dict\n",
    "\n",
    "answer_no_null = answer.fillna(\n",
    "    {\"duration_commercial\": 0, \"duration_total\": 0, \"commercial_ratio\": 0}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0f46ba-4d94-4775-a452-b02ec7749937",
   "metadata": {},
   "source": [
    "## Putting everyting together: develop an end-to-end program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404b24c3-ddfa-4395-a903-2c854dabd345",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#  commercials.py #############################################################\n",
    "#\n",
    "# This program computes the commercial ratio for each channel present in the\n",
    "# dataset.\n",
    "#\n",
    "###############################################################################\n",
    "\n",
    "import os\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\n",
    "    \"Getting the Canadian TV channels with the highest/lowest proportion of commercials.\"\n",
    ").getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "###############################################################################\n",
    "# Reading all the relevant data sources\n",
    "###############################################################################\n",
    "\n",
    "# change the account name to your email account\n",
    "account='sli'\n",
    "\n",
    "# define a root path to access the data in the DataAnalysisWithPythonAndPySpark\n",
    "root_path='/net/clusterhn/home/'+account+'/isa460/Data/'\n",
    "\n",
    "DIRECTORY = root_path+'/broadcast_logs/'\n",
    "\n",
    "logs = spark.read.csv(\n",
    "    os.path.join(DIRECTORY, \"BroadcastLogs_2018_Q3_M8_sample.CSV\"),\n",
    "    sep=\"|\",\n",
    "    header=True,\n",
    "    inferSchema=True,\n",
    "    timestampFormat=\"yyyy-MM-dd\"\n",
    ")\n",
    "\n",
    "log_identifier = spark.read.csv(\n",
    "    os.path.join(DIRECTORY, \"ReferenceTables/LogIdentifier.csv\"),\n",
    "    sep=\"|\",\n",
    "    header=True,\n",
    "    inferSchema=True,\n",
    ")\n",
    "\n",
    "cd_category = spark.read.csv(\n",
    "    os.path.join(DIRECTORY, \"ReferenceTables/CD_Category.csv\"),\n",
    "    sep=\"|\",\n",
    "    header=True,\n",
    "    inferSchema=True,\n",
    ").select(\n",
    "    \"CategoryID\",\n",
    "    \"CategoryCD\",\n",
    "    F.col(\"EnglishDescription\").alias(\"Category_Description\"),\n",
    ")\n",
    "\n",
    "cd_program_class = spark.read.csv(\n",
    "    os.path.join(DIRECTORY, \"ReferenceTables/CD_ProgramClass.csv\"),\n",
    "    sep=\"|\",\n",
    "    header=True,\n",
    "    inferSchema=True,\n",
    ").select(\n",
    "    \"ProgramClassID\",\n",
    "    \"ProgramClassCD\",\n",
    "    F.col(\"EnglishDescription\").alias(\"ProgramClass_Description\"),\n",
    ")\n",
    "\n",
    "###############################################################################\n",
    "# Data processing\n",
    "###############################################################################\n",
    "\n",
    "logs = logs.drop(\"BroadcastLogID\", \"SequenceNO\")\n",
    "\n",
    "logs = logs.withColumn(\n",
    "    \"duration_seconds\",\n",
    "    (\n",
    "        F.col(\"Duration\").substr(1, 2).cast(\"int\") * 60 * 60\n",
    "        + F.col(\"Duration\").substr(4, 2).cast(\"int\") * 60\n",
    "        + F.col(\"Duration\").substr(7, 2).cast(\"int\")\n",
    "    ),\n",
    ")\n",
    "\n",
    "log_identifier = log_identifier.where(F.col(\"PrimaryFG\") == 1)\n",
    "\n",
    "logs_and_channels = logs.join(log_identifier, \"LogServiceID\")\n",
    "\n",
    "full_log = logs_and_channels.join(cd_category, \"CategoryID\", how=\"left\").join(\n",
    "    cd_program_class, \"ProgramClassID\", how=\"left\"\n",
    ")\n",
    "\n",
    "full_log.groupby(\"LogIdentifierID\").agg(\n",
    "    F.sum(\n",
    "        F.when(\n",
    "            F.trim(F.col(\"ProgramClassCD\")).isin(\n",
    "                [\"COM\", \"PRC\", \"PGI\", \"PRO\", \"LOC\", \"SPO\", \"MER\", \"SOL\"]\n",
    "            ),\n",
    "            F.col(\"duration_seconds\"),\n",
    "        ).otherwise(0)\n",
    "    ).alias(\"duration_commercial\"),\n",
    "    F.sum(\"duration_seconds\").alias(\"duration_total\"),\n",
    ").withColumn(\n",
    "    \"commercial_ratio\", F.col(\"duration_commercial\") / F.col(\"duration_total\")\n",
    ").orderBy(\n",
    "    \"commercial_ratio\", ascending=False\n",
    ").show(\n",
    "    10, False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2df36b8-c8f9-4d51-a4ba-4cb861348272",
   "metadata": {},
   "source": [
    "## In Class Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6429eb77-ca25-4bdc-84cf-95a038737acb",
   "metadata": {},
   "source": [
    "### Exercise 5.5\n",
    "\n",
    "Using the data from the data/broadcast_logs/Call_Signs.csv (careful: the delimiter here is the comma, not the pipe!), add the Undertaking_Name to our final table to display a human-readable description of the channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20baf34-986e-48ab-a953-d15fe5cda172",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34d4bc4-7613-4cd9-aaff-c6b710476e5d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30eddca6-d5d7-431d-849a-20946914d6af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6edd6dab-9933-454b-a65e-56d0690d9da7",
   "metadata": {},
   "source": [
    "### Exercise 5.6\n",
    "\n",
    "The government of Canada is asking for your analysis, but they’d like the PRC to be weighted differently. They’d like each PRC second to be considered 0.75 commercial seconds. Modify the program to account for this change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4d6f15-b988-4e6c-913c-b4a5ba7eaa23",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7925f94-69da-42e7-b221-d6b9416f5ae7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Big Data Analytics",
   "language": "python",
   "name": "bigdataanalytics"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
